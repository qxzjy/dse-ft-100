{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse de Données Complète\n",
    "\n",
    "## **I. Introduction**\n",
    "### **1.1. Contexte de l'analyse**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2. Objectifs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3. Questions de recherche**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Importations**\n",
    "### **2.1. Importation des Librairies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import the secondary libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2. Importation des jeux de données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a Pandas DataFrame\n",
    "df = pd.read_csv('your_dataset.csv') # To read CSV file\n",
    "\n",
    "# Read data from a URL\n",
    "#df_url = pd.read_csv('https://example.com/your_data.csv') # To read data from a URL\n",
    "\n",
    "# Read Excel file\n",
    "#df_excel = pd.read_excel('your_file.xlsx', sheet_name='Sheet1') # To read Excel file\n",
    "\n",
    "# Read JSON file\n",
    "#df_json = pd.read_json('your_file.json') # To read JSON file\n",
    "\n",
    "# Read SQL File\n",
    "#from sqlalchemy import create_engine # To insert in the secondary libraries\n",
    "#engine = create_engine('sqlite:///your_database.db')      ### Create a SQLite database engine\n",
    "#df_sql = pd.read_sql('SELECT * FROM your_table', engine)  ### Read data from a SQL table\n",
    "\n",
    "# Read Parquet file\n",
    "#df_parquet = pd.read_parquet('your_file.parquet') # To read Parquet file\n",
    "\n",
    "# The file type above are the main file types you will encounter in Data Science. But here are some other file types below such as :\n",
    "\n",
    "# Read HDF5 file\n",
    "#df_hdf5 = pd.read_hdf('your_file.h5', key='your_key') # To read HDF5 file\n",
    "\n",
    "# Read CERN ROOT file\n",
    "#from rootpy.io import ROOTFile # To insert in the secondary libraries\n",
    "#df_root = ROOT.TFile.Open('your_file.root') # To read ROOT file\n",
    "\n",
    "# Read Feather file\n",
    "#df_feather = pd.read_feather('your_file.feather') # To read Feather file\n",
    "\n",
    "# Read fixed-width file\n",
    "#column_widths = [10, 15, 20]    ## To define column widths\n",
    "#df_fixed_width = pd.read_fwf('your_file.txt', widths=column_widths) # To read fixed-width file\n",
    "\n",
    "# Read data from the clipboard\n",
    "#df_clipboard = pd.read_clipboard() # To read data from the clipboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **III. Exploration Initiale des Données/Dataset Exploratory Analysis**\n",
    "### **3.1. Vue d'ensemble des données (Dimensions, infos, en-tête du jeu de données)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dataframe dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the information of the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 5 rows of the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2. Nettoyage des données/Data Cleaning (valeurs manquantes, lignes dupliquées, réduction de données et valeurs aberrantes)**\n",
    "**Toujours procéder à chaque étape en enregistrant dans une nouvelle variable**\n",
    "#### **3.2.1. Deal with missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the potentially missing values and check their appartenance in the dataframe (Either Numerical or Categorical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For both numerical and categorical missing values, the solutions are from the best to the worst\n",
    "# Deal with potentially categorical missing values with\n",
    "# 1. Create a new category for missing values like \"noclue/missing/unknown\", to introduce a new dimension, better fitting reality\n",
    "\n",
    "# 2.5. If it's boolean type, check this : https://github.com/travisjungroth/trinary#examples and that discussion : https://www.reddit.com/r/Python/comments/zudwr6/trinary_a_python_project_for_threevalued_logic_it/\n",
    "\n",
    "# 2. Drop rows with missing values\n",
    "\n",
    "# Deal with potentially numerical missing values with\n",
    "# 1. Replace missing values with 0 and add  a new column to show the missing values (where 0 = no missing value, 1 = missing value), again, new dimension\n",
    "\n",
    "# 2. Replace missing values with mode/median (if not normally distributed data) \n",
    "\n",
    "# 3. Replace missing values with mean (if normally distributed data)\n",
    "\n",
    "# 4. Drop rows with missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with a high percentage of missing values (e.g. more than 60% threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.2. Deal with duplicated rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the potentially duplicated rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the potentially duplicated rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.3. Deal with constant columns values, low-variances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique and distinct values of the columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns with constant values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for predictions, put aside low-variance columns, inducing overfitting (threshold = 0.1 is an enough low-variance threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.4. Deal with outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Handling Outliers Using the Interquartile Range (IQR):\n",
    "\n",
    "# Calculate the Interquartile Range (IQR)\n",
    "\n",
    "# Deal with outliers based on IQR, with a threshold of 1.5 times the IQR with :\n",
    "# 1. Trimming weight (to reduce the weight of outliers)\n",
    "\n",
    "# 2. Changing the scale (Winsorisation, Imputation, Trimming to reduce the values)\n",
    "\n",
    "# 3. Using M-estimation (a robust estimation technique)\n",
    "\n",
    "# 4. Removing outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plots for Outlier Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2.5. Transform data types**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print columns data types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionnary or a list of the columns to be converted to either Numeric(uint8, int8, uint16, int16, uint32, int32, uint64, int64) or Categorical type(category, bool), descriptive types (object, str) or datetime types (date, datetime or even the conversion to ordinal type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the conversion to the columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3. Feature Engineering**\n",
    "***Ce sont des exemples extrêmement utiles, mais certains d'entre eux ne sont pas des nécessités***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Creating New Features:\n",
    "\n",
    "# Create a new feature by adding two existing features\n",
    "\n",
    "# Create a new feature by multiplying two existing features\n",
    "\n",
    "# Create a new feature by calculating the mean of a group\n",
    "\n",
    "# Binning/Discretization\n",
    "\n",
    "# 2. Transforming Existing Features:\n",
    "\n",
    "# Log-transform a numeric feature\n",
    "\n",
    "# Min-Max scaling\n",
    "\n",
    "# One-hot encoding for a categorical feature to create dummy variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3 subsets \n",
    "# 1. numerical one\n",
    "\n",
    "# 2. categorical one\n",
    "\n",
    "# 3. descriptive one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Phrase de conclusion sur le nombre de lignes supprimées, de valeurs remplacées, de réduction du poids des données et du jeu de données**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **IV- Analyse Exploratoire des Données/Exploratory Data Analysis**\n",
    "### **4.1. Univariate Analysis**\n",
    "#### **4.1.1. Quantitative Descriptive Statistical Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the statistical summary of the variables (Descriptive Statistics of Dispersion Indicators (Range, Variance, Standard Deviation) & Position Indicators (Sum, Median & Quartiles, Mean, Minimum, Maximum, Mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the Numerical Data Distribution with an histogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.1.2. Qualitative Descriptive Statistical Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of the Categorical Data Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the sums/counts with categorical variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2. Bivariate/Multivariate Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a nice chart with one or two numerical and one categorical variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a pair plots with multiple numeric variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a chart with multiple categorical variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot for a categorical variable vs. a numeric variable (distribution of categorical across numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a violin plot for a categorical variable vs. a numeric variable (distribution of categorical across numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot for numeric-numeric relationships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and print a cross tabulation for categorical-numerical relationships\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vous avez techniquement terminé les séries d'analyses statistiques**  \n",
    "**Maintenant, il faut poser la conclusion des analyses précédentes, car par la suite, il va falloir supposer, proposer des hypothèses, à partir des conclusions**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **V. Analyse Inférentielle**\n",
    "### **5.1. Hypothesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2. Hypothesis tests**\n",
    "#### **5.2.1. Simple procedure tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-Value\n",
    "\n",
    "# U-Value\n",
    "\n",
    "# Z-Value\n",
    "\n",
    "# Confidence Interval\n",
    "\n",
    "# F-Value\n",
    "\n",
    "# P-Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binominal Classification, to be used for a lone categorical variable with 2 values (0 and 1, True and False, Man and Woman, Male and Female...)\n",
    "\n",
    "# Mann-Whitney U Test\n",
    "\n",
    "# Wilcoxon Rank Sum Test\n",
    "\n",
    "# Chi-Square Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.2.2. Multicollinearity test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print tolerance value (Value has to be over 0.1 for a non critic, but over 0.3 is already a good value)\n",
    "\n",
    "# Print VIF (Variance Inflation Factor), the lower the better (Over 10 is a critical value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.2.3. Correlations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print correlation matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use different correlation metrics such as:\n",
    "# Pearson's correlation coefficient as a Linear correlation between continuous variables (NOT SUITABLE FOR DISCRETE VARIABLES, IT REQUIRES A NORMAL DISTRIBUTION)\n",
    "\n",
    "# Spearman's rho as a Non-parametric correlation between ordinal variables (DOESN'T REQUIRES A NORMAL DISTRIBUTION)\n",
    "\n",
    "# Kendall's tau is a Non-parametric correlation between ordinal variables (DOESN'T REQUIRES A NORMAL DISTRIBUTION, AND FIT LOW-SIZE DATASETS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.2.4. Variance Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA (Variance Analysis of One Factor), used to test if there is a significant difference between the means of two or more groups.\n",
    "\n",
    "# MANOVA (Multivariate Analysis of Variance, Of Multiple Factors), used to test if there is a significant difference between the means of multiple groups. (REQUIRES NORMAL DISTRIBUTION)\n",
    "\n",
    "# Kruskal-Wallis H-test, used to test if there is a significant difference between the means of multiple groups. (DOESN'T REQUIRES NORMAL DISTRIBUTION)\n",
    "\n",
    "# Friedman Test (Used when normality and homogeneity of variance assumption is violated), used to test if there is a significant difference between the means of multiple groups. (DOESN'T REQUIRES NORMAL DISTRIBUTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now you can \"assume\" a causality IN THIS CELL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **VI- Analyse prédictive**\n",
    "### **6.1. Preprocessus des données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separe the target variable from the features with Y and X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can try with both separately, but at least have one feature set with nominal data such as date or datetime to be converted as an ordinal one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical features to dummy variables with One-hot encoding such as dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6.1.1. Création des Features/Target & Mises à l'echelle, Normalisation, Transformation, Train-Test-Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler/transform for a numeric feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6.1.2. Préparation des modèles et des validateurs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Estimators Map](https://scikit-learn.org/stable/_downloads/b82bf6cd7438a351f19fac60fbc0d927/ml_map.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check SKLearn models map for a better understanding of what fit the best \n",
    "# But the best is to create a list of multiple models and compare their performances throught validation tests\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.2. Prédiction des données**\n",
    "#### **6.2.1. Execution des modèles sur les données d'entrainement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the list or model, to return the prediction of y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6.2.2. Vérification des erreurs et/ou de la précision des modèles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression validations such as MAE, MAPE, MSE, RMSE, R², Adjusted R²... Those are the 5 essentials metrics (MAPE is the MAE in %)\n",
    "\n",
    "# Classification validations such as Confusion Matrix, Accuracy, Precision, Recall, F1-Score... Those also are the 5 essentials metrics\n",
    "\n",
    "# Clustering validations such as KMeans, Meanshift, DBSCAN, BIRCH, Silhouette Coefficient, Indexes (Rand, Dunn, VI)... Those again are the 5 essentials metrics (Meanshift is a alternative version of KMeans)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **6.2.3. Visualisation par graphique des modèles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use same charts than before to compare visually the data (you can also add the target variable before retiring it to check the differences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.2.4. Execution des modèles sur de potentiels jeux de données aux Target manquante**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate what you did in the training model but to fit the \"real-life\" data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **VII - Analyse Prescriptive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What CAN be done following the results of the model ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis of the dataset with target included\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart visualisation of the dataset with target included\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Following the visualisations, what SHOULD be done ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Give advices, insights etc...**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonCassandra",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
